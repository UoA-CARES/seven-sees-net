{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0845abc0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dba1256f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadat/miniconda3/envs/dataloader/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sadat/miniconda3/envs/dataloader/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import transforms\n",
    "from model.mmcv_csn import ResNet3dCSN\n",
    "from model.cls_head import ClassifierHead\n",
    "from model.pose_encoder import PoseEncoder\n",
    "from model.scheduler import GradualWarmupScheduler\n",
    "from model.multimodal_neck import MultiModalNeck\n",
    "from mmaction.datasets import build_dataset\n",
    "from dataset.dataset import MultiModalDataset\n",
    "from dataset.transforms import transform\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc31acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "except:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1326b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = 'work_dirs/wlasl-dataset/'\n",
    "batch_size = 2\n",
    "\n",
    "os.makedirs(work_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d6785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transform()\n",
    "\n",
    "train_dataset = MultiModalDataset(ann_file='data/wlasl10/train_annotations.txt',\n",
    "                            root_dir='data/wlasl10/rawframes',\n",
    "                            clip_len=32,\n",
    "                            resolution=224,\n",
    "                            transforms = transforms,\n",
    "                            frame_interval=1,\n",
    "                            num_clips=1\n",
    "                            )\n",
    "\n",
    "test_dataset = MultiModalDataset(ann_file='data/wlasl10/test_annotations.txt',\n",
    "                            root_dir='data/wlasl10/rawframes',\n",
    "                            clip_len=32,\n",
    "                            resolution=224,\n",
    "                            transforms = transforms,\n",
    "                            frame_interval=1,\n",
    "                            num_clips=1\n",
    "                            )\n",
    "\n",
    "# Setting up dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True,\n",
    "                                    num_workers=4,\n",
    "                                    pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                    batch_size=1,\n",
    "                                    shuffle=True,\n",
    "                                    num_workers=4,\n",
    "                                    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63618ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 256\n",
      "186 186\n",
      "186 186\n",
      "186 186\n",
      "256 256\n",
      "256 256\n",
      "256 256\n",
      "256 256\n",
      "256 256\n",
      "256 256\n",
      "256 256256 \n",
      "256\n",
      "256 256\n",
      "203 203\n",
      "203 203\n",
      "203 203\n",
      "256 256\n",
      "207 207\n",
      "207 207\n",
      "207 207\n",
      "256 256\n",
      "216 216\n",
      "256 256\n",
      "216 216\n",
      "203 204216\n",
      " 216\n",
      "203 204\n",
      "203 204\n",
      "256 256\n",
      "215 214\n",
      "215 214\n",
      "215 214\n",
      "256 256\n",
      "180 181\n",
      "180 181\n",
      "180 181\n"
     ]
    }
   ],
   "source": [
    "rgb, _, face, left_hand, right_hand, depth, flow, pose,  label = next(iter(test_loader)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96650c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-12 12:13:30,667 - model - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth\n",
      "2023-02-12 12:13:30,668 - model - INFO - load checkpoint from http path: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth\n",
      "2023-02-12 12:13:30,699 - model - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth\n",
      "2023-02-12 12:13:30,700 - model - INFO - load checkpoint from http path: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth\n",
      "2023-02-12 12:13:30,726 - model - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth\n",
      "2023-02-12 12:13:30,726 - model - INFO - load checkpoint from http path: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth\n",
      "2023-02-12 12:13:30,752 - model - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth\n",
      "2023-02-12 12:13:30,752 - model - INFO - load checkpoint from http path: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth\n",
      "2023-02-12 12:13:30,777 - model - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth\n",
      "2023-02-12 12:13:30,778 - model - INFO - load checkpoint from http path: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth\n",
      "2023-02-12 12:13:30,803 - model - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth\n",
      "2023-02-12 12:13:30,804 - model - INFO - load checkpoint from http path: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth\n"
     ]
    }
   ],
   "source": [
    "from model.seven_seas_net import SevenSeesNet\n",
    "\n",
    "model = SevenSeesNet()\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1efcfeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model(rgb=rgb,\n",
    "         depth=depth,\n",
    "         flow=flow,\n",
    "         face=face,\n",
    "         left_hand=left_hand,\n",
    "         right_hand=right_hand,\n",
    "         pose=pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2bed53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0764a276",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1a02e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=0.000125, momentum=0.9, weight_decay=0.00001)\n",
    "\n",
    "# Specify Loss\n",
    "loss_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify total epochs\n",
    "epochs = 100\n",
    "\n",
    "# Specify learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=120, gamma=0.1)\n",
    "\n",
    "scheduler_steplr = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[34, 84], gamma=0.1)\n",
    "scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=16, after_scheduler=scheduler_steplr)\n",
    "\n",
    "# Specify Loss\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52bdaca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_accuracy(scores, labels, topk=(1, )):\n",
    "    \"\"\"Calculate top k accuracy score.\n",
    "    Args:\n",
    "        scores (list[np.ndarray]): Prediction scores for each class.\n",
    "        labels (list[int]): Ground truth labels.\n",
    "        topk (tuple[int]): K value for top_k_accuracy. Default: (1, ).\n",
    "    Returns:\n",
    "        list[float]: Top k accuracy score for each k.\n",
    "    \"\"\"\n",
    "    res = np.zeros(len(topk))\n",
    "    labels = np.array(labels)[:, np.newaxis]\n",
    "    for i, k in enumerate(topk):\n",
    "        max_k_preds = np.argsort(scores, axis=1)[:, -k:][:, ::-1]\n",
    "        match_array = np.logical_or.reduce(max_k_preds == labels, axis=1)\n",
    "        topk_acc_score = match_array.sum() / match_array.shape[0]\n",
    "        res[i] = topk_acc_score\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch_index, interval=5):\n",
    "    \"\"\"Run one epoch for training.\n",
    "    Args:\n",
    "        epoch_index (int): Current epoch.\n",
    "        interval (int): Frequency at which to print logs.\n",
    "    Returns:\n",
    "        last_loss (float): Loss value for the last batch.\n",
    "    \"\"\"\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, (rgb, _, face, left_hand, right_hand, depth, flow, pose, targets) in enumerate(train_loader):\n",
    "        rgb, face, left_hand, right_hand, depth, flow, pose, targets = rgb.to(device), face.to(device), left_hand.to(device), right_hand.to(device), depth.to(device), flow.to(device), pose.to(device), targets.to(device)\n",
    "#         rgb = rgb.reshape((-1, ) + rgb.shape[2:])\n",
    "#         face = face.reshape((-1, ) + face.shape[2:])\n",
    "#         flow = flow.reshape((-1, ) + flow.shape[2:])\n",
    "#         left_hand = left_hand.reshape((-1, ) + left_hand.shape[2:])\n",
    "#         right_hand = right_hand.reshape((-1, ) + right_hand.shape[2:])\n",
    "#         depth = depth.reshape((-1, ) + depth.shape[2:])\n",
    "        \n",
    "        targets = targets.reshape(-1, )\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(rgb=rgb,\n",
    "                         depth=depth,\n",
    "                         flow=flow,\n",
    "                         face=face,\n",
    "                         left_hand=left_hand,\n",
    "                         right_hand=right_hand,\n",
    "                         pose=pose)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), max_norm=40, norm_type=2.0)\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % interval == interval-1:\n",
    "            last_loss = running_loss / interval  # loss per batch\n",
    "            print(\n",
    "                f'Epoch [{epoch_index}][{i+1}/{len(train_loader)}], lr: {scheduler.get_last_lr()[0]:.5e}, loss: {last_loss:.5}')\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss, scheduler.get_last_lr()[0]\n",
    "\n",
    "\n",
    "def validate():\n",
    "    \"\"\"Run one epoch for validation.\n",
    "    Returns:\n",
    "        avg_vloss (float): Validation loss value for the last batch.\n",
    "        top1_acc (float): Top-1 accuracy in decimal.\n",
    "        top5_acc (float): Top-5 accuracy in decimal.\n",
    "    \"\"\"\n",
    "    running_vloss = 0.0\n",
    "    running_vacc = np.zeros(2)\n",
    "\n",
    "    print('Evaluating top_k_accuracy...')\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i, (rgb, _, face, left_hand, right_hand, depth, flow, pose, targets)  in enumerate(test_loader):\n",
    "            rgb, face, left_hand, right_hand, depth, flow, pose, targets = rgb.to(device), face.to(device), left_hand.to(device), right_hand.to(device), depth.to(device), flow.to(device), pose.to(device), targets.to(device)\n",
    "            rgb = rgb.reshape((-1, ) + rgb.shape[2:])\n",
    "            face = face.reshape((-1, ) + face.shape[2:])\n",
    "            flow = flow.reshape((-1, ) + flow.shape[2:])\n",
    "            left_hand = left_hand.reshape((-1, ) + left_hand.shape[2:])\n",
    "            right_hand = right_hand.reshape((-1, ) + right_hand.shape[2:])\n",
    "            depth = depth.reshape((-1, ) + depth.shape[2:])\n",
    "            \n",
    "            targets = targets.reshape(-1, )\n",
    "\n",
    "            outputs = model(rgb=rgb,\n",
    "                             depth=depth,\n",
    "                             flow=flow,\n",
    "                             face=face,\n",
    "                             left_hand=left_hand,\n",
    "                             right_hand=right_hand,\n",
    "                             pose=pose)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            running_vloss += loss\n",
    "\n",
    "            running_vacc += top_k_accuracy(outputs.detach().cpu().numpy(),\n",
    "                                           targets.detach().cpu().numpy(), topk=(1, 5))\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "\n",
    "    acc = running_vacc/len(test_loader)\n",
    "    top1_acc = acc[0].item()\n",
    "    top5_acc = acc[1].item()\n",
    "\n",
    "    return (avg_vloss, top1_acc, top5_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e452239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 256\n",
      "256 256256\n",
      " 256\n",
      "218 204217 \n",
      "204\n",
      "202218  202217\n",
      "\n",
      "204 204\n",
      "202 202\n",
      "218 217204\n",
      " 204\n",
      "202 202\n",
      "256 256\n",
      "210 210\n",
      "210 210\n",
      "210 210\n",
      "256 256\n",
      "191 191\n",
      "256 191256\n",
      " 191\n",
      "189 191189 \n",
      "191\n",
      "189 189\n",
      "189 189\n",
      "256 256\n",
      "229 229\n",
      "229 229\n",
      "229 229\n",
      "256 256\n",
      "202 203\n",
      "202 203\n",
      "202 203\n",
      "256 256\n",
      "163 162\n",
      "163 162\n",
      "256 256\n",
      "189 189\n",
      "189 189\n",
      "189 189\n",
      "256 256\n",
      "212 211\n",
      "212 211\n",
      "212 211\n",
      "256 256\n",
      "205 205\n",
      "205 205\n",
      "205 205\n",
      "256 256\n",
      "196 196\n",
      "196 196\n",
      "196 196\n",
      "256 256\n",
      "256 256\n",
      "256 256\n",
      "256 256\n",
      "256 256\n",
      "188 188\n",
      "188 188\n",
      "256 256188\n",
      " 188\n",
      "204 205\n",
      "204 205\n",
      "204 205\n",
      "256 256\n",
      "256 256\n",
      "198 198213 \n",
      "212\n",
      "213 198212\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 3.\nOriginal Traceback (most recent call last):\n  File \"/home/sadat/miniconda3/envs/dataloader/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/sadat/miniconda3/envs/dataloader/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/sadat/miniconda3/envs/dataloader/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/sadat/Desktop/seven-sees-net/dataset/dataset.py\", line 204, in __getitem__\n    results = self.transforms(results)\n  File \"/home/sadat/Desktop/seven-sees-net/dataset/transforms.py\", line 172, in __call__\n    frames = self.crop(frames, 'right_hand')\n  File \"/home/sadat/Desktop/seven-sees-net/dataset/transforms.py\", line 156, in crop\n    padimage[x0:x1,y0:y1] = img\nValueError: could not broadcast input array from shape (163,162,3) into shape (162,162,3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (rgb, _, face, left_hand, right_hand, depth, flow, pose, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m      2\u001b[0m         rgb, face, left_hand, right_hand, depth, flow, pose, targets \u001b[38;5;241m=\u001b[39m rgb\u001b[38;5;241m.\u001b[39mto(device), face\u001b[38;5;241m.\u001b[39mto(device), left_hand\u001b[38;5;241m.\u001b[39mto(device), right_hand\u001b[38;5;241m.\u001b[39mto(device), depth\u001b[38;5;241m.\u001b[39mto(device), flow\u001b[38;5;241m.\u001b[39mto(device), pose\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/dataloader/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/dataloader/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1313\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1312\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1316\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n",
      "File \u001b[0;32m~/miniconda3/envs/dataloader/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1359\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/dataloader/lib/python3.8/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 3.\nOriginal Traceback (most recent call last):\n  File \"/home/sadat/miniconda3/envs/dataloader/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/sadat/miniconda3/envs/dataloader/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/sadat/miniconda3/envs/dataloader/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/sadat/Desktop/seven-sees-net/dataset/dataset.py\", line 204, in __getitem__\n    results = self.transforms(results)\n  File \"/home/sadat/Desktop/seven-sees-net/dataset/transforms.py\", line 172, in __call__\n    frames = self.crop(frames, 'right_hand')\n  File \"/home/sadat/Desktop/seven-sees-net/dataset/transforms.py\", line 156, in crop\n    padimage[x0:x1,y0:y1] = img\nValueError: could not broadcast input array from shape (163,162,3) into shape (162,162,3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 198\n",
      "213 212\n",
      "198 198\n",
      "256 256\n",
      "186 186\n",
      "186 186\n",
      "186 186\n",
      "256 256\n",
      "235 236\n",
      "235 236\n",
      "235 236\n",
      "256 256\n",
      "198 199\n",
      "198 199\n",
      "198 199\n",
      "256 256\n",
      "207 206\n",
      "207 206\n",
      "207 206\n",
      "256 256\n",
      "206 207\n",
      "206 207\n",
      "206 207\n",
      "256 256\n",
      "183 183\n",
      "183 183\n",
      "183 183\n",
      "256 256\n",
      "256 256\n",
      "199 199209\n",
      " 208\n",
      "199 199\n",
      "209 208\n",
      "199 199\n",
      "209 208\n",
      "256 256\n",
      "197 196\n",
      "197 196\n",
      "197 196\n",
      "256 256\n",
      "189 189\n",
      "189 189\n",
      "189 189\n",
      "256 256\n",
      "192 192\n",
      "192 192\n",
      "192 192\n",
      "256 256\n",
      "188 187\n",
      "188 187\n",
      "188 187\n",
      "256 256\n",
      "198 198\n",
      "198 198\n",
      "198 198\n"
     ]
    }
   ],
   "source": [
    "for i, (rgb, _, face, left_hand, right_hand, depth, flow, pose, targets) in enumerate(train_loader):\n",
    "        rgb, face, left_hand, right_hand, depth, flow, pose, targets = rgb.to(device), face.to(device), left_hand.to(device), right_hand.to(device), depth.to(device), flow.to(device), pose.to(device), targets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06329753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Loop\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "# Transfer model to device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Turn on gradient tracking and do a forward pass\n",
    "    model.train(True)\n",
    "    avg_loss, learning_rate = train_one_epoch(epoch+1)\n",
    "\n",
    "    # Turn off  gradients for reporting\n",
    "    model.train(False)\n",
    "\n",
    "#     avg_vloss, top1_acc, top5_acc = validate()\n",
    "\n",
    "#     print(\n",
    "#         f'top1_acc: {top1_acc:.4}, top5_acc: {top5_acc:.4}, train_loss: {avg_loss:.5}, val_loss: {avg_vloss:.5}')\n",
    "\n",
    "#     # Track best performance, and save the model's state\n",
    "#     if avg_vloss < best_vloss:\n",
    "#         best_vloss = avg_vloss\n",
    "#         model_path = work_dir + f'epoch_{epoch+1}.pth'\n",
    "#         print(f'Saving checkpoint at {epoch+1} epochs...')\n",
    "#         torch.save(model.state_dict(), model_path)\n",
    "\n",
    "     # Adjust learning rate\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b67fef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc56900",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rgb_encoder(rgb)[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "        for i, (rgb, _, face, left_hand, right_hand, depth, flow, pose, targets)  in enumerate(train_loader):\n",
    "            rgb, face, left_hand, right_hand, depth, flow, pose, targets = rgb.to(device), face.to(device), left_hand.to(device), right_hand.to(device), depth.to(device), flow.to(device), pose.to(device), targets.to(device)\n",
    "            rgbt = rgb.reshape((-1, ) + rgb.shape[2:])\n",
    "            face = face.reshape((-1, ) + face.shape[2:])\n",
    "            flow = flow.reshape((-1, ) + flow.shape[2:])\n",
    "            left_hand = left_hand.reshape((-1, ) + left_hand.shape[2:])\n",
    "            right_hand = right_hand.reshape((-1, ) + right_hand.shape[2:])\n",
    "            depth = depth.reshape((-1, ) + depth.shape[2:])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f0aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee1ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgbt.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataloader",
   "language": "python",
   "name": "dataloader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
